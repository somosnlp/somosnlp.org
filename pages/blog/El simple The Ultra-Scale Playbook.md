**El simple The Ultra-Scale Playbook**

Entrenar Modelos de Lenguaje Grandes (LLMs) como los que vemos hoy en día requiere una cantidad enorme de poder computacional y memoria. Si bien empezar con una sola GPU es el primer paso, escalar a miles de GPUs presenta desafíos significativos, especialmente problemas de memoria. Este artículo explora cómo abordamos estos desafíos utilizando diversas técnicas de paralelismo y optimización.

**El Proceso Básico de Entrenamiento y el Problema de la Memoria**

En esencia, el entrenamiento de un modelo en una sola GPU consta de tres pasos:

1. **Paso hacia adelante (Forward Pass):** La entrada pasa por el modelo para obtener una salida.
2. **Paso hacia atrás (Backward Pass):** Se calculan los gradientes, que indican cómo ajustar los parámetros del modelo.
3. **Paso de optimización (Optimization):** Se usan los gradientes para actualizar los parámetros del modelo.

El problema es que, a medida que los modelos y los tamaños de los lotes (batch sizes) crecen, la memoria de la GPU se agota. 

**¿Qué consume esta memoria?&#32;**

Principalmente, **los parámetros del modelo (weights), los gradientes (gradients), los estados del optimizador (optimizer states) y las activaciones (activations)**. Los parámetros, gradientes y estados del optimizador pueden sumar gigabytes rápidamente para modelos grandes. Las activaciones, que son los resultados intermedios del paso hacia adelante necesarios para calcular gradientes durante el paso hacia atrás, son particularmente problemáticas porque su memoria requerida escala con la longitud de la secuencia y el tamaño del lote (batch size).

El tamaño del lote (batch size) es un hiperparámetro clave. Se reporta comúnmente en tokens, y para LLMs recientes, un punto ideal está en el rango de 4-60 millones de tokens por lote. Lograr tamaños de lote tan grandes con memoria limitada es uno de los primeros retos.

**Primeras Técnicas para Manejar la Memoria**

1. **Recomputación de Activaciones (Activation Recomputation):** Para "domar" la "explosión de activaciones", podemos optar por no almacenar todas las activaciones del paso hacia adelante. En su lugar, algunas se recomputan durante el paso hacia atrás cuando son necesarias. La estrategia "full" recomputa casi todo, ahorrando mucha memoria pero aumentando el costo computacional. Estrategias "selective" recomputan solo las activaciones más costosas de almacenar. Aunque aumenta un poco los FLOPS (operaciones de punto flotante), reduce significativamente los accesos a memoria, lo que a menudo resulta en una computación más rápida en GPUs. FlashAttention, una optimización común para el mecanismo de atención, integra la recomputación.
2. **Acumulación de Gradientes (Gradient Accumulation):** Esta es una técnica sencilla para simular un lote grande dividiéndolo en "micro-lotes". Realizamos pasos hacia adelante y hacia atrás para cada micro-lote, acumulando los gradientes. Solo después de procesar todos los micro-lotes, realizamos un único paso de optimización con los gradientes acumulados. Esto nos permite usar un tamaño de lote efectivo (global batch size) mayor del que cabría en la memoria de una sola GPU, a costa de tiempo de cálculo adicional por la sequentialidad.

**Escalando a Múltiples GPUs: Paralelismo**

Cuando una sola GPU ya no es suficiente, necesitamos distribuir el entrenamiento a través de múltiples dispositivos. Aquí entran las técnicas de paralelismo:

1. **Paralelismo de Datos (Data Parallelism - DP):** Esta es una extensión natural de la acumulación de gradientes, pero paralela. El modelo se replica en varias GPUs. Cada GPU procesa un micro-lote de datos diferente en paralelo. Para mantener las réplicas del modelo sincronizadas, los gradientes calculados en cada GPU se promedian usando una operación llamada "all-reduce" antes del paso del optimizador. Es crucial solapar (overlap) la computación del paso hacia atrás con la comunicación del "all-reduce" para no dejar las GPUs inactivas. DP es la primera dimensión del paralelismo (1D). La desventaja principal es que la memoria por GPU sigue siendo la misma.
2. **ZeRO (Zero Redundancy Optimizer):** DP ayuda a escalar el *tamaño del lote*, pero si el modelo *en sí mismo* no cabe en una sola GPU, necesitamos otra solución. ZeRO aborda esto eliminando la redundancia en los parámetros, gradientes y estados del optimizador a través de las réplicas de DP. En lugar de que cada GPU almacene copias completas de estos elementos, ZeRO los "shardea" (divide) entre las GPUs en el grupo de DP. ZeRO-3 es la etapa más agresiva, shardeando parámetros, gradientes y estados del optimizador. Esto reduce significativamente la memoria necesaria por GPU para estos elementos. Sin embargo, ZeRO **no sharde las activaciones**.
3. **Paralelismo de Tensores (Tensor Parallelism - TP):** Para sharder también las activaciones, usamos TP. TP divide los tensores (como las matrices de pesos y las activaciones) en fragmentos más pequeños a lo largo de alguna dimensión (por ejemplo, la dimensión oculta) y distribuye estos fragmentos a diferentes GPUs. Las operaciones como la multiplicación de matrices se realizan en paralelo en los fragmentos. Esto requiere comunicación entre GPUs para ciertas operaciones, como "all-gather" o "reduce-scatter", que a veces son difíciles de solapar completamente. TP reduce la memoria para parámetros, gradientes, estados del optimizador y, crucialmente, **activaciones**. Sin embargo, ciertas operaciones como LayerNorm y Dropout requieren acceso a la dimensión oculta completa, necesitando reagrupar activaciones.
4. **Paralelismo de Secuencia (Sequence Parallelism - SP):** SP es un complemento natural a TP. Aborda las operaciones que TP no sharde completamente, como LayerNorm y Dropout. En lugar de sharder a lo largo de la dimensión oculta como TP, SP sharde las activaciones de estas operaciones a lo largo de la dimensión de la *secuencia de entrada*. Esto permite reducir aún más la memoria de activación máxima, ayudando a acomodar lotes y secuencias más grandes. SP se usa junto con TP.
5. **Paralelismo de Contexto (Context Parallelism - CP):** Dirigido a entrenar con secuencias *muy* largas (por ejemplo, 128k+ tokens). CP también sharde la secuencia de entrada a lo largo de las GPUs, similar a SP, pero se aplica a módulos donde ya se usa TP. El desafío principal es la capa de atención, ya que cada token necesita información de otros tokens en la secuencia completa. Una técnica clave para manejar la comunicación de atención en CP es "Ring Attention", que distribuye el cálculo de atención e intercambio de datos clave/valor (KV) en un patrón de anillo eficiente. CP ayuda a reducir la memoria de activación para secuencias largas.
6. **Paralelismo de Pipeline (Pipeline Parallelism - PP):** PP es útil cuando el modelo es tan grande que no cabe en la memoria de un solo *nodo* (un grupo de GPUs con interconexión rápida, como 8 GPUs en un servidor). PP divide las *capas* del modelo a través de diferentes GPUs o nodos. Por ejemplo, las capas 1-4 en GPU 1, las capas 5-8 en GPU 2, y así sucesivamente. Esto distribuye la memoria necesaria para los parámetros del modelo. La comunicación en PP consiste en pasar las activaciones de las últimas capas de una GPU a las primeras capas de la siguiente GPU en el pipeline. El principal desafío es el "bubble" o tiempo inactivo que ocurre porque las GPUs deben esperar las activaciones de la GPU anterior para empezar su trabajo. Se han desarrollado varios "horarios" o esquemas de PP (como one-forward-one-backward, interleaved stages) para reducir este bubble y mejorar la eficiencia. PP tiende a escalar mejor a través de nodos que TP debido a menores requisitos de ancho de banda de interconexión.
7. **Paralelismo de Expertos (Expert Parallelism - EP):** Específico para arquitecturas de modelos "Mixture-of-Experts" (MoE). En modelos MoE, algunas capas tienen múltiples "expertos" (módulos de feedforward paralelos) y se enrutan tokens a uno o varios expertos. EP consiste en colocar estos expertos en diferentes GPUs. Como los expertos operan de forma independiente en un token dado, esto es relativamente sencillo comparado con TP, requiriendo principalmente enrutar los "hidden states" de los tokens al experto correcto usando una operación "all-to-all". EP a menudo se combina con DP. Permite escalar la capacidad del modelo al tener muchos expertos distribuidos.

**Combinando y Optimizando**

Estas técnicas de paralelismo a menudo se combinan (formando lo que se llama "paralelismo 5D"). Por ejemplo, se puede usar TP dentro de un nodo (donde la comunicación es rápida), y PP o ZeRO-3 a través de nodos (donde la comunicación es más lenta). CP se añade para secuencias muy largas y EP para modelos MoE.

Además de la organización del paralelismo, la eficiencia también depende de optimizaciones de bajo nivel en la GPU:

- **Fused Kernels:** Combinar varias operaciones sucesivas en una sola tarea para la GPU reduce la sobrecarga y el movimiento de datos entre memorias.
- **Flash Attention:** Un ejemplo maestro de un kernel fusionado que optimiza el cálculo de atención al evitar materializar matrices grandes en la memoria global lenta de la GPU.
- **Entrenamiento de Precisión Mixta (Mixed Precision Training):** Usar formatos numéricos de menor precisión (como BF16 o el experimental FP8) en lugar de FP32 para los cálculos del paso hacia adelante/atrás. Esto reduce el uso de memoria y permite usar hardware optimizado (Tensor Cores), aunque requiere técnicas para mantener la estabilidad numérica (como escalamiento de pérdida y acumulación en mayor precisión). FP8 promete mayor velocidad pero presenta desafíos de estabilidad.

Encontrar la configuración óptima de paralelismo, tamaños de lote y micro-lotes, y optimizaciones de bajo nivel es un proceso complejo que a menudo requiere experimentar y hacer benchmarks (mediciones de rendimiento) en el clúster de GPUs específico. No hay una única "bala de plata"; la mejor configuración depende del tamaño del modelo, el tamaño del clúster y la arquitectura de red.

En resumen, entrenar LLMs gigantes es un arte y una ciencia que combina estrategias de alto nivel para distribuir el trabajo y la memoria a través de miles de GPUs con optimizaciones de bajo nivel para exprimir cada ciclo de la GPU. No hay una única "solución" el arte recabe en elegir la mejor configuración para conseguir el mejor LLM, un LLM que se diferencie de los demás.