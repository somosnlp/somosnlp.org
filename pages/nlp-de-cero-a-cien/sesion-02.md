---
title: "Sesión 2: Modelos Secuenciales (RNNs y LSTMs)"
date: 2021-07-27T16:00:00.000+00:00
lang: es
duration: 40min
---

<div>
    <CourseSummary
        description="En esta sesión explicaremos la arquitectura de los modelos secuenciales (RNNs y LSTMs) y cómo aplicarlos a casos prácticos como la resolución de un problema de clasificación."
        video="https://www.youtube.com/embed/mTbY5BF7Y5o"
        slides="https://github.com/somosnlp/nlp-de-cero-a-cien/blob/main/2_modelos_secuenciales/modelos_secuenciales.pdf"
        name="María Grandury"
        twitter="https://twitter.com/mariagrandury"
        linkedin="https://www.linkedin.com/in/mariagrandury"
        github="https://github.com/mariagrandury"
    />
</div>

---

## Notebook

<a href="https://colab.research.google.com/drive/1Nk9bQ2BTymXwANYw5V2O0iTYR01ptccv" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

## Recursos adicionales
- [Understanding The Recurrent Neural Network | Post by Amit Shekhar](https://medium.com/mindorks/understanding-the-recurrent-neural-network-44d593f112a2)
- [Recurrent Neural Networks and LSTM explained | Post by Purnasai Gudikandula](https://purnasaigudikandula.medium.com/recurrent-neural-networks-and-lstm-explained-7f51c7f6bbb9)
- [Understanding LSTM and its diagrams | Post by Shi Yan](https://blog.mlreview.com/understanding-lstm-and-its-diagrams-37e2f46f1714)
-  [How do LSTM networks solve the Problem of Vanishing Gradients | Post by Nir Arbel](https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577#:~:text=However%2C%20RNNs%20suffer%20from%20the,no%20real%20learning%20is%20done)
- [How RNNs and LSTMs work | Video by Brandon Rohrer](https://www.youtube.com/watch?v=WCUNPb-5EYI)

## Papers
- [Long Short-Term Memory](https://doi.org/10.1162/neco.1997.9.8.1735): paper donde se introdujeron las LSTMs.

---

Siguiente sesión: [Arquitectura Transformer](/nlp-de-cero-a-cien/sesion-03)
