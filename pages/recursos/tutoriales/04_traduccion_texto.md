---
title: Traducci√≥n de Texto
description: C√≥mo afinar un modelo de T5 en espa√±ol para traduccir frases del espa√±ol al portugu√©s
cover: https://somosnlp.github.io/assets/images/ilustraciones/undraw_education_edited.svg
author: Oscar Cumbicus
bio: Profesor @UNL e Investigador @IxaGroup
twitter: https://twitter.com/OscarCumbicus
linkedin: https://www.linkedin.com/in/oscar-cumbicus-47095443/
github: https://github.com/oskrmiguel
---

<a href="https://colab.research.google.com/github/oskrmiguel/Traductor_T5_Spanish_to_Portuguese/blob/main/T5_Espa%C3%B1ol_a_Portugues.ipynb
" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

## Configuraci√≥n
Este cuaderno es una adaptaci√≥n de [How to fine-tune a model on
translation](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/translation.ipynb)



Si vas a abrir este cuaderno en colab, probablemente necesitar√°s
instalar ü§ó Transformers y ü§ó Datasets. Descomenta la siguiente celda y
ejec√∫tala.



``` {.python}
! pip install datasets transformers[sentencepiece] sacrebleu
```



Si est√°s abriendo este cuaderno localmente, aseg√∫rate de que tu entorno
tiene una instalaci√≥n de la √∫ltima versi√≥n de esas bibliotecas.

Para poder compartir tu modelo con la comunidad y generar resultados
como el que se muestra en la imagen de abajo a trav√©s de la API de
inferencia, hay que seguir algunos pasos m√°s.

Primero tienes que almacenar tu token de autenticaci√≥n del sitio web de
Hugging Face (¬°reg√≠strate [aqu√≠](https://huggingface.co/join) si a√∫n no
lo has hecho!) y luego ejecutar la siguiente celda e introducir tu
nombre de usuario y contrase√±a:



``` {.python}
from huggingface_hub import notebook_login

notebook_login()
```



A continuaci√≥n, debe instalar Git-LFS. Descomente las siguientes
instrucciones:



``` {.python}
!apt install git-lfs
```



Aseg√∫rate de que tu versi√≥n de Transformers es al menos la 4.11.0 ya que
la funcionalidad se introdujo en esa versi√≥n:



``` {.python}
import transformers

print(transformers.__version__)
```


    4.17.0




# Fine-tuning de un modelo en una tarea de traducci√≥n



En este cuaderno, veremos c√≥mo afinar uno de los modelos [ü§ó
Transformers](https://github.com/huggingface/transformers) para una
tarea de traducci√≥n. Utilizaremos el [tatoeba
dataset](https://huggingface.co/datasets/tatoeba), es una colecci√≥n de frases y traducciones.


Veremos c√≥mo cargar f√°cilmente el conjunto de datos para esta tarea
utilizando ü§ó Datasets y c√≥mo afinar un modelo sobre √©l utilizando la API
`Trainer`.



## Cargar la base de datos



Utilizaremos la biblioteca [ü§ó
Datasets](https://github.com/huggingface/datasets) para descargar los
datos y obtener la m√©trica que necesitamos utilizar para la evaluaci√≥n
(para comparar nuestro modelo con el benchmark). Esto se puede hacer
f√°cilmente con las funciones `load_dataset` y `load_metric`. Aqu√≠
utilizamos la parte espa√±ol/portugu√©s del conjunto de datos tatoeba.



```python
from datasets import load_dataset, load_metric,DatasetDict

raw_datasets = load_dataset("tatoeba", "es-pt")
metric = load_metric("sacrebleu")
```


El objeto `dataset` es
[`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict),
que contiene una clave para el conjunto de entrenamiento



```python
raw_datasets
```


    DatasetDict({
        train: Dataset({
            features: ['id', 'translation'],
            num_rows: 67782
        })
    })




Para acceder a un elemento real, primero hay que seleccionar una
divisi√≥n y luego dar un √≠ndice:



```python
raw_datasets["train"][0]
```


    {'id': '0',
     'translation': {'es': '¬°Intentemos algo!',
      'pt': 'Vamos tentar alguma coisa!'}}




Nosotros dividiremos este conjunto de datos en entrenamiento, validaci√≥n
y pruebas



```python
train_devtest = raw_datasets['train'].train_test_split(shuffle = True, seed = 200, test_size=0.1)
posts_dev_test = train_devtest['test'].train_test_split(shuffle = True, seed = 200, test_size=0.50)
raw_datasets = DatasetDict({
    'train': train_devtest['train'],
    'validation': posts_dev_test['test'],
    'test': posts_dev_test['train']})
```



```python
raw_datasets
```


    DatasetDict({
        train: Dataset({
            features: ['id', 'translation'],
            num_rows: 61003
        })
        validation: Dataset({
            features: ['id', 'translation'],
            num_rows: 3390
        })
        test: Dataset({
            features: ['id', 'translation'],
            num_rows: 3389
        })
    })




## Preprocesar los datos



Antes de que podamos introducir esos textos en nuestro modelo, tenemos
que preprocesarlos. Esto lo hace un `Tokenizer` de Transformers ü§ó que
(como su nombre indica) tokenizar√° las entradas (incluyendo la
conversi√≥n de los tokens a sus correspondientes IDs en el vocabulario
preentrenado) y las pondr√° en un formato que el modelo espera, as√≠ como
generar√° las otras entradas que el modelo requiere.

Para hacer todo esto, instanciamos nuestro tokenizador con el m√©todo
`AutoTokenizer.from_pretrained`, que asegurar√°

-   obtenemos un tokenizador que corresponde a la arquitectura del
    modelo que queremos utilizar,
-   descargamos el vocabulario utilizado al preentrenar este punto de
    control espec√≠fico.

Ese vocabulario se almacenar√° en la cach√©, para que no se descargue de
nuevo la pr√≥xima vez que ejecutemos la c√©lula.



```python
from transformers import AutoTokenizer
model_checkpoint='t5-small'
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```
Si est√° utilizando uno de los cinco puntos de control T5 que requieren
un prefijo especial para poner antes de las entradas, debe adaptar la
siguiente celda

```python
if model_checkpoint in ["t5-small", "t5-base", "t5-larg", "t5-3b", "t5-11b"]:
    prefix = "translate Spanish to Portuguese: "
else:
    prefix = ""
```

Podemos entonces escribir la funci√≥n que preprocesar√° nuestras muestras.
Simplemente las alimentamos al `tokenizer` con el argumento
`truncation=True`. Esto asegurar√° que una entrada m√°s larga de lo que el
modelo seleccionado puede manejar se truncar√° a la longitud m√°xima
aceptada por el modelo. El relleno se tratar√° m√°s tarde (en un
recopilador de datos), de modo que rellenamos los ejemplos hasta la
longitud m√°s larga del lote y no todo el conjunto de datos.



```python
max_input_length = 128
max_target_length = 128
source_lang = "es"
target_lang = "pt"

def preprocess_function(examples):
    inputs = [prefix + ex[source_lang] for ex in examples["translation"]]
    targets = [ex[target_lang] for ex in examples["translation"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Setup the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```



Esta funci√≥n funciona con uno o varios ejemplos. En el caso de varios
ejemplos, el tokenizador devolver√° una lista de listas para cada clave:



```python
preprocess_function(raw_datasets['train'][:2])
```


    {'input_ids': [[13959, 5093, 12, 21076, 10, 1174, 15, 6626, 2561, 23, 32, 3, 9, 1313, 15, 3, 9, 3151, 35, 355, 7, 20, 1413, 15785, 9, 7, 12, 26, 32, 7, 10381, 3, 26, 2, 9, 7, 5, 1], [13959, 5093, 12, 21076, 10, 1626, 75, 154, 6899, 238, 285, 1498, 7, 975, 3, 154, 40, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[465, 7, 7, 32, 353, 44, 7253, 3151, 35, 9, 7, 20, 1413, 15785, 9, 7, 12, 26, 32, 7, 3, 32, 7, 1227, 9, 7, 5, 1], [1699, 11666, 3, 32, 238, 285, 7, 49, 3, 287, 3, 400, 5, 1]]}




Para aplicar esta funci√≥n en todos los pares de frases de nuestro
conjunto de datos, s√≥lo tenemos que utilizar el m√©todo `map` de nuestro
objeto `dataset` que hemos creado anteriormente. Esto aplicar√° la
funci√≥n en todos los elementos de todas las divisiones en `dataset`, por
lo que nuestros datos de entrenamiento, validaci√≥n y prueba ser√°n
preprocesados en un solo comando.



```python
tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
```

Incluso mejor, los resultados son autom√°ticamente almacenados en cach√©
por la biblioteca ü§ó Datasets para evitar perder tiempo en este paso la
pr√≥xima vez que ejecutes tu cuaderno. La biblioteca ü§ó Datasets es
normalmente lo suficientemente inteligente como para detectar cuando la
funci√≥n que pasas a map ha cambiado (y por lo tanto requiere no utilizar
los datos de la cach√©). Por ejemplo, detectar√° correctamente si cambias
la tarea en la primera celda y vuelves a ejecutar el cuaderno. ü§ó
Datasets te avisa cuando utiliza archivos en cach√©, puedes pasar
`load_from_cache_file=False` en la llamada a `map` para no utilizar los
archivos en cach√© y forzar que se aplique de nuevo el preprocesamiento.

Tenga en cuenta que pasamos `batched=True` para codificar los textos por
lotes juntos. Esto es para aprovechar todas las ventajas del tokenizador
r√°pido que cargamos antes, que utilizar√° el multithreading para tratar
los textos de un lote de forma concurrente.



## Fine-tune el modelo



Ahora que nuestros datos est√°n listos, podemos descargar el modelo
preentrenado y ajustarlo. Como nuestra tarea es del tipo secuencia a
secuencia, utilizamos la clase `AutoModelForSeq2SeqLM`. Al igual que con
el tokenizador, el m√©todo `from_pretrained` descargar√° y almacenar√° en
cach√© el modelo por nosotros.



```python
from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```



Para instanciar un `Seq2SeqTrainer`, necesitaremos definir tres cosas
m√°s. La m√°s importante es el
[`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments),
que es una clase que contiene todos los atributos para personalizar el
entrenamiento. Requiere un nombre de carpeta, que se utilizar√° para
guardar los puntos de control del modelo, y todos los dem√°s argumentos
son opcionales:



```python
batch_size = 32
model_name = model_checkpoint.split("/")[-1]
args = Seq2SeqTrainingArguments(
    f"{model_name}-finetuned-{source_lang}-to-{target_lang}",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=1,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
```



Aqu√≠ configuramos la evaluaci√≥n para que se realice al final de cada
√©poca, ajustamos la tasa de aprendizaje, utilizamos el `tama√±o_del_lote`
definido en la parte superior de la celda y personalizamos el
decaimiento del peso. Como el `Seq2SeqTrainer` guardar√° el modelo
regularmente y nuestro conjunto de datos es bastante grande, le decimos
que haga tres guardados como m√°ximo. Por √∫ltimo, utilizamos la opci√≥n
`predict_with_generate` (para generar correctamente los res√∫menes) y
activamos el entrenamiento de precisi√≥n mixta (para ir un poco m√°s
r√°pido).

El √∫ltimo argumento para configurar todo para que podamos empujar el
modelo al [Hub](https://huggingface.co/models) regularmente durante el
entrenamiento. Elim√≠nalo si no has seguido los pasos de instalaci√≥n en
la parte superior del cuaderno. Si quieres guardar tu modelo localmente
con un nombre diferente al del repositorio al que ser√° empujado, o si
quieres empujar tu modelo bajo una organizaci√≥n y no tu espacio de
nombres, utiliza el argumento `hub_model_id` para establecer el nombre
del repositorio (tiene que ser el nombre completo, incluyendo tu espacio
de nombres: por ejemplo `"sgugger/marian-finetuned-en-to-ro"` o
`"huggingface/marian-finetuned-en-to-ro"`).

A continuaci√≥n, necesitamos un tipo especial de recopilador de datos,
que no s√≥lo rellenar√° las entradas hasta la longitud m√°xima en el lote,
sino tambi√©n las etiquetas:



```python
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```



Lo √∫ltimo que hay que definir para nuestro `Seq2SeqTrainer` es c√≥mo
calcular las m√©tricas a partir de las predicciones. Tenemos que definir
una funci√≥n para esto, que s√≥lo utilizar√° la \"m√©trica\" que cargamos
antes, y tenemos que hacer un poco de pre-procesamiento para decodificar
las predicciones en los textos:



```python
import numpy as np

def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]

    return preds, labels

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    result = {"bleu": result["score"]}

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result["gen_len"] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    return result
```



A continuaci√≥n, s√≥lo tenemos que pasar todo esto junto con nuestros
conjuntos de datos al `Seq2SeqTrainer`:



```python
trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
```




Ahora podemos afinar nuestro modelo simplemente llamando al m√©todo
`train`:



```python
trainer.train()
```

Ahora puedes subir el resultado del entrenamiento al Hub, s√≥lo tienes
que ejecutar esta instrucci√≥n:



```python
trainer.push_to_hub()
```



## Pruebas del Modelo



Ahora probaremos nuestro modelo en la traducci√≥n con el conjunto de
test.

Primero descargaremos nuestro modelo subido al hub o si lo tienes
localmente utiliza la ruta donde guardaste el modelo reci√©n entrenado.



```python
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
model = AutoModelForSeq2SeqLM.from_pretrained("oskrmiguel/t5-small-finetuned-es-to-pt")
tokenizer = AutoTokenizer.from_pretrained("oskrmiguel/t5-small-finetuned-es-to-pt")
```



Ahora utilizamos la divisi√≥n \'test\' que hicimos de nuestro DataSet
principal.

Como ejemplo traduciremos las 10 primeras frases del test.



```python
for indice in range(10):
  inputs = tokenizer(
      "{} {}".format("translate Spanish to Portuguese: ",raw_datasets['test'][indice]['translation']['es']),
      return_tensors="pt"
      )
  outputs = model.generate(inputs["input_ids"], max_length=40, num_beams=4, early_stopping=True)
  print("{} {}".format(raw_datasets['test'][indice]['translation']['es'],tokenizer.decode(outputs[0])))
```


    Lo que queda es recuerdo y amargura en el coraz√≥n. <pad> O que queda √© recuerdo e amargura no coraz<unk>o.</s>
    La prostaglandina es una sustancia que protege las paredes del aparato digestivo. <pad> A prostaglandina √© uma sustancia que protege as paredes do aparato digestivo.</s>
    Naoki y Kaori tienen la misma edad. <pad> Naoki e Kaori tem a misma edade.</s>
    ¬øTen√©s un papelito para anotar el n√∫mero? <pad> Voc√™ tenho um papelito para anotar o n<unk>mero?</s>
    La esperanza muere al final. <pad> A esperanza muito final.</s>
    Este es un tipo de madera dif√≠cil de serrar. <pad> Este √© um tipo de madera dif<unk>cil de serrar.</s>
    Este personaje s√≥lo viste de rosa. <pad> Este personaje s√≥ viste de rosa.</s>
    √âl vive solito. <pad> Ele vive solito.</s>
    Qu√© suerte que haga tan buen tiempo. <pad> Qu√© suerte que haga t<unk>o buen tempo.</s>
    Mar√≠a no sabe que es portadora del gen del albinismo. <pad> Mar<unk>a n<unk>o sabe que √© portadora do gen do albinismo.</s>




**NOTA**: Puedes entrenar el modelo por m√°s √©pocas para que obtengas
mejores resultados en tu traducci√≥n

